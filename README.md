# ğŸ” Niche Search Engine - Job Aggregator

A high-performance job search engine built in Rust using a workspace architecture. Scrapes remote job listings from WeWorkRemotely.com, indexes them with Tantivy full-text search, and serves results via a REST API.

---

## ğŸ¯ What is a Niche Search Engine?

A **Niche Search Engine** is a specialized search engine focused on a specific domain or topic, unlike general-purpose search engines (Google, Bing) that index the entire web.

### Comparison: General vs Niche Search Engine

| Aspect | General Search Engine | Niche Search Engine (This Project) |
|--------|----------------------|-----------------------------------|
| **Scope** | Entire internet (billions of pages) | Single domain: remote jobs |
| **Data Source** | Web crawlers indexing everything | Targeted scraping of WeWorkRemotely |
| **Index Size** | Petabytes of data | Megabytes (hundreds/thousands of jobs) |
| **Relevance** | Broad ranking algorithms | Domain-specific (title, company, salary) |
| **Infrastructure** | Massive distributed systems | Single server, local index |
| **Use Case** | Find anything | Find remote programming jobs |

### Why Build a Niche Search Engine?

1. **Focused Results** - No noise from unrelated content
2. **Custom Schema** - Index fields relevant to your domain (salary, location)
3. **Fast & Lightweight** - Small index = millisecond queries
4. **Control** - You decide what gets indexed and how
5. **Learning** - Understand search engine internals without massive scale

### How This Project Works as a Niche Search Engine

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        NICHE SEARCH ENGINE PIPELINE                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   [WeWorkRemotely.com]                                                  â”‚
â”‚          â”‚                                                              â”‚
â”‚          â–¼                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Instead of crawling the entire web,               â”‚
â”‚   â”‚  SCRAPER    â”‚    we only fetch job listings from ONE source.       â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚
â”‚          â”‚                                                              â”‚
â”‚          â–¼                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Instead of generic text, we extract               â”‚
â”‚   â”‚  INDEXER    â”‚    structured data: title, company, salary.          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚
â”‚          â”‚                                                              â”‚
â”‚          â–¼                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    Instead of PageRank, we use                       â”‚
â”‚   â”‚  SEARCHER   â”‚    BM25 relevance on job-specific fields.            â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§  How the Indexer Works (Deep Dive)

The indexer is the **heart of any search engine**. It transforms raw data into a structure optimized for fast searching.

### What is an Inverted Index?

An **Inverted Index** is the core data structure used by search engines. Instead of storing "document â†’ words", it stores "word â†’ documents".

**Example: Traditional Storage (Forward Index)**
```
Job 1: "Senior Rust Developer at TechCorp"
Job 2: "Python Backend Engineer"
Job 3: "Rust Systems Programmer"
```

**Inverted Index (What Tantivy Builds)**
```
"rust"     â†’ [Job 1, Job 3]
"senior"   â†’ [Job 1]
"python"   â†’ [Job 2]
"backend"  â†’ [Job 2]
"techcorp" â†’ [Job 1]
```

When you search for "rust", the engine instantly finds Jobs 1 and 3 without scanning every document!

### Tantivy Indexing Process (Step-by-Step)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TANTIVY INDEXING PIPELINE                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 1: SCHEMA DEFINITION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Define what fields exist and their types:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Field       â”‚ Type     â”‚ Options                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ title       â”‚ TEXT     â”‚ STORED (returned in results)            â”‚
â”‚ company     â”‚ TEXT     â”‚ STORED (returned in results)            â”‚
â”‚ description â”‚ TEXT     â”‚ NOT STORED (searchable only, saves RAM) â”‚
â”‚ salary_min  â”‚ I64      â”‚ INDEXED (for numeric range queries)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Step 2: TOKENIZATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Break text into searchable tokens:

"Senior Rust Developer" â†’ ["senior", "rust", "developer"]
                              â”‚
                    (lowercase, remove punctuation)


Step 3: BUILD INVERTED INDEX
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
For each token, record which documents contain it:

Token "rust":
  â”œâ”€â”€ Document ID: 0  (position: 2, field: title)
  â”œâ”€â”€ Document ID: 5  (position: 1, field: title)
  â””â”€â”€ Document ID: 12 (position: 4, field: description)

Token "developer":
  â”œâ”€â”€ Document ID: 0  (position: 3, field: title)
  â””â”€â”€ Document ID: 8  (position: 1, field: title)


Step 4: COMMIT TO DISK
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Write index segments to ./search_index/:

search_index/
â”œâ”€â”€ meta.json           â† Index metadata (schema, segments)
â”œâ”€â”€ .managed.json       â† Tantivy internal tracking
â””â”€â”€ <segment_id>/       â† Actual index data
    â”œâ”€â”€ .fast           â† Fast fields (numeric data)
    â”œâ”€â”€ .idx            â† Inverted index
    â”œâ”€â”€ .pos            â† Token positions
    â”œâ”€â”€ .store          â† Stored field values
    â””â”€â”€ .term           â† Term dictionary
```

### How Search Queries Work

When you query `/search?q=rust developer`:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       SEARCH FLOW                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. PARSE QUERY
   "rust developer" â†’ [Term("rust"), Term("developer")]

2. LOOKUP INVERTED INDEX
   rust      â†’ [doc0, doc5, doc12]
   developer â†’ [doc0, doc8]

3. COMBINE RESULTS (intersection/union based on query type)
   Matching docs: [doc0, doc5, doc8, doc12]

4. SCORE WITH BM25
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Doc ID  â”‚ BM25 Score                                      â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ doc0    â”‚ 15.32 (contains BOTH terms in title)            â”‚
   â”‚ doc5    â”‚ 8.21  (contains "rust" only)                    â”‚
   â”‚ doc12   â”‚ 5.67  (contains "rust" in description)          â”‚
   â”‚ doc8    â”‚ 4.12  (contains "developer" only)               â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

5. RETURN TOP K RESULTS (sorted by score)
   â†’ doc0, doc5, doc12, doc8 (top 10)
```

### BM25 Scoring Algorithm

Tantivy uses **BM25** (Best Match 25), the industry-standard ranking algorithm:

```
BM25(doc, query) = Î£ IDF(term) Ã— TF(term, doc) Ã— (kâ‚ + 1)
                   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                   TF(term, doc) + kâ‚ Ã— (1 - b + b Ã— |doc|/avgdl)

Where:
- IDF = Inverse Document Frequency (rare words score higher)
- TF  = Term Frequency (more occurrences = higher score)
- kâ‚  = Term saturation parameter (default: 1.2)
- b   = Length normalization (default: 0.75)
```

**In simple terms:** Documents score higher when they:
1. Contain rare/unique query terms (IDF)
2. Contain query terms multiple times (TF)
3. Are shorter (normalized by document length)

---

## ğŸ“ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Rust Workspace                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   common/   â”‚      scraper/       â”‚          server/            â”‚
â”‚  (Library)  â”‚     (Binary)        â”‚         (Binary)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Job struct  â”‚ HTTP Client         â”‚ Tantivy Indexer             â”‚
â”‚ Serde       â”‚ HTML Parser         â”‚ Axum Web Server             â”‚
â”‚             â”‚ Regex Extractor     â”‚ Search API                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Workflow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   1. SCRAPER     â”‚â”€â”€â”€â”€â–¶â”‚   2. INDEXER     â”‚â”€â”€â”€â”€â–¶â”‚   3. SERVER      â”‚
â”‚                  â”‚     â”‚                  â”‚     â”‚                  â”‚
â”‚ Fetch HTML from  â”‚     â”‚ Read jobs.json   â”‚     â”‚ Accept HTTP      â”‚
â”‚ WeWorkRemotely   â”‚     â”‚ Build Tantivy    â”‚     â”‚ search queries   â”‚
â”‚ Parse listings   â”‚     â”‚ schema & index   â”‚     â”‚ Return JSON      â”‚
â”‚ Extract salary   â”‚     â”‚ Commit to disk   â”‚     â”‚ results          â”‚
â”‚ Save to JSON     â”‚     â”‚                  â”‚     â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                        â”‚                        â”‚
        â–¼                        â–¼                        â–¼
   data/jobs.json          search_index/           http://127.0.0.1:3000
```

---

## ğŸ“¦ Module Details

### 1. Common Library (`common/`)

Shared data structures used by both scraper and server.

**File:** `common/src/lib.rs`

```rust
pub struct Job {
    pub title: String,       // Job title
    pub company: String,     // Company name
    pub location: String,    // Location/Region
    pub description: String, // Job description text
    pub salary_raw: String,  // Original salary text from website
    pub salary_min: Option<i64>, // Extracted minimum salary (if found)
    pub url: String,         // Direct link to job posting
}
```

**Dependencies:**
- `serde` - Serialization/deserialization for JSON

---

### 2. Scraper Binary (`scraper/`)

Web scraper that collects job listings from WeWorkRemotely.

**File:** `scraper/src/main.rs`

**Features:**
| Feature | Description |
|---------|-------------|
| Multi-URL Scraping | Scrapes 4 job category pages in sequence |
| Deduplication | Uses HashSet to prevent duplicate jobs |
| Salary Extraction | Regex-based extraction of salary numbers |
| Error Resilience | Continues to next URL if one fails |

**Target URLs:**
1. `/remote-software-developer-jobs`
2. `/categories/remote-full-stack-programming-jobs`
3. `/categories/remote-back-end-programming-jobs`
4. `/categories/remote-front-end-programming-jobs`

**CSS Selectors Used:**
```
Job Container:  li.feature, .new-listing-container
Title:          .new-listing__header__title
Company:        .new-listing__company-name
Location:       .new-listing__company-headquarters
Link:           .listing-link--unlocked, ._blank
```

**Salary Extraction Logic:**
```rust
fn extract_salary(salary_raw: &str) -> Option<i64>
// Regex: \$?(\d{1,3}(?:,\d{3})*|\d+)
// Matches: "$50,000", "100000", "$75,000 - $99,999"
// Returns first number >= 1000 (filters out noise like "21d")
```

**Output:** `data/jobs.json`

**Dependencies:**
- `reqwest` (blocking) - HTTP client
- `scraper` - HTML parsing
- `regex` - Salary extraction
- `serde_json` - JSON serialization

---

### 3. Server Binary (`server/`)

Search engine server combining Tantivy indexing with Axum REST API.

**File:** `server/src/main.rs`

#### Tantivy Indexer

**Schema Definition:**
| Field | Type | Options | Purpose |
|-------|------|---------|---------|
| `title` | TEXT | STORED | Searchable, returned in results |
| `company` | TEXT | STORED | Searchable, returned in results |
| `description` | TEXT | (not stored) | Searchable only, saves disk space |
| `salary_min` | I64 | INDEXED | For range filtering (future use) |

**Index Location:** `./search_index/`

**Indexing Process:**
1. Read `data/jobs.json` on startup
2. Create/open Tantivy index directory
3. Clear existing documents (fresh re-index)
4. Add all jobs to index with 50MB writer heap
5. Commit changes to disk

#### Axum Web Server

**Endpoints:**
| Method | Path | Description |
|--------|------|-------------|
| GET | `/` | API info and usage help |
| GET | `/search?q=<keywords>` | Full-text job search |

**Search Response Format:**
```json
{
  "query": "rust developer",
  "total_results": 5,
  "results": [
    {
      "title": "Senior Rust Developer",
      "company": "TechCorp",
      "score": 12.345
    }
  ]
}
```

**Query Parser Configuration:**
- Searches across: `title` + `description` fields
- Returns: Top 10 results by relevance score
- Shared state via `Arc<AppState>` containing IndexReader

**Dependencies:**
- `tantivy` 0.19 - Full-text search engine
- `axum` 0.6 - Async web framework
- `tokio` - Async runtime
- `serde_json` - JSON responses

---

## ğŸš€ Quick Start

### Prerequisites
- Rust 1.70+ with Cargo
- Internet connection (for scraping)

### Step 1: Clone & Build
```bash
git clone <repository-url>
cd Niche-Search-Engine-Job-Aggregator
cargo build --workspace
```

### Step 2: Scrape Jobs
```bash
cargo run -p scraper@0.1.0
```
Output:
```
ğŸ” Starting WeWorkRemotely Job Scraper...
ğŸ“¡ Fetching jobs from: https://weworkremotely.com/...
âœ… Fetched 77130 bytes
ğŸ“‹ Found: Senior Backend Engineer at TechCorp
...
ğŸ“Š Total unique jobs found: 45
ğŸ’¾ Saved 45 jobs to "data/jobs.json"
```

### Step 3: Start Search Server
```bash
cargo run -p server@0.1.0
```
Output:
```
ğŸš€ Starting Job Search Engine Server...
ğŸ“‚ Loading jobs from "data/jobs.json"
ğŸ“Š Loaded 45 jobs
ğŸ“ Creating new index...
âœ… Indexing complete!
ğŸŒ Server running at http://127.0.0.1:3000
```

### Step 4: Search Jobs
```bash
# Search for Rust jobs
curl "http://127.0.0.1:3000/search?q=rust"

# Search for Python backend
curl "http://127.0.0.1:3000/search?q=python+backend"
```

---

## ğŸ“ Project Structure

```
Niche-Search-Engine-Job-Aggregator/
â”œâ”€â”€ Cargo.toml              # Workspace manifest
â”œâ”€â”€ Cargo.lock              # Dependency lock file
â”œâ”€â”€ README.md               # This file
â”‚
â”œâ”€â”€ common/                 # Shared library
â”‚   â”œâ”€â”€ Cargo.toml
â”‚   â””â”€â”€ src/
â”‚       â””â”€â”€ lib.rs          # Job struct definition
â”‚
â”œâ”€â”€ scraper/                # Web scraper binary
â”‚   â”œâ”€â”€ Cargo.toml
â”‚   â””â”€â”€ src/
â”‚       â””â”€â”€ main.rs         # Scraping logic
â”‚
â”œâ”€â”€ server/                 # Search API binary
â”‚   â”œâ”€â”€ Cargo.toml
â”‚   â””â”€â”€ src/
â”‚       â””â”€â”€ main.rs         # Indexer + Axum server
â”‚
â”œâ”€â”€ data/                   # Generated data (gitignored)
â”‚   â””â”€â”€ jobs.json           # Scraped job listings
â”‚
â””â”€â”€ search_index/           # Tantivy index (gitignored)
    â”œâ”€â”€ meta.json
    â””â”€â”€ *.managed.json
```

---

## âš™ï¸ Configuration

### Dependency Constraints
These versions are specifically chosen to avoid Windows/C++ compilation issues:

```toml
# Tantivy with minimal features
tantivy = { version = "0.19", default-features = false, features = ["mmap", "stopwords"] }

# Axum compatible with Tantivy 0.19
axum = "0.6"

# Blocking HTTP for simplicity
reqwest = { version = "0.11", features = ["blocking", "json"] }
```

---

## ğŸ”§ Development

### Run Tests
```bash
cargo test --workspace
```

### Check Linting
```bash
cargo clippy --workspace
```

### Format Code
```bash
cargo fmt --all
```

---

## ğŸ“ Notes

- **Rate Limiting:** WeWorkRemotely may block aggressive scraping. Add delays between requests if needed.
- **Index Persistence:** The `search_index/` directory persists between runs. Delete it to force re-indexing.
- **Schema Changes:** If you modify the Tantivy schema, delete `search_index/` before restarting the server.

---

## ğŸ“„ License

MIT License - See LICENSE file for details.
